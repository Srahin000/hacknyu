# NPU Deployment Checklist

## ğŸ“‹ Your To-Do List

### â˜ Step 1: Download Model (5-10 minutes)

**Action:** Download Llama-3.2-1B ONNX from Qualcomm AI Hub

1. Open: https://aihub.qualcomm.com/models
2. Search: "Llama-3.2-1B" or "Llama"
3. Click on model
4. Download ONNX format (~1-2GB)
5. Save to: `models\llama_npu\model.onnx`

**Check status:**
```powershell
python check_model_ready.py
```

---

### â˜ Step 2: Deploy to NPU (10-30 minutes)

**Action:** Compile model for Snapdragon NPU

```powershell
python deploy_fixed.py --model models\llama_npu\model.onnx
```

**What happens:**
- Uploads to Qualcomm cloud âœ“
- Compiles for NPU (this is slow!) â³
- Downloads compiled binary âœ“
- Saves to `deployed_models/`

**This takes 10-30 minutes - be patient!**

---

### â˜ Step 3: Test Harry with NPU (instant!)

**Action:** Chat with super-fast Harry

```powershell
python talk_to_harry_npu.py
```

**Expected:**
- Response time: ~300-500ms âš¡
- 10-12x faster than CPU!
- Same Harry personality

---

## ğŸ” Check Your Progress

```powershell
# Quick status check
python check_model_ready.py
```

This shows:
- âœ… or âŒ Model downloaded
- âœ… or â³ Model deployed
- Next steps

---

## â±ï¸ Time Estimate

| Step | Time | What You Do |
|------|------|-------------|
| Download | 5-10 min | Wait for download |
| Deploy | 10-30 min | Run command, wait |
| Test | Instant | Chat with Harry! |
| **TOTAL** | **20-40 min** | Mostly waiting |

---

## ğŸ¯ Current Status

Run this command to see where you are:

```powershell
python check_model_ready.py
```

Output will show:
```
âœ… ONNX model downloaded (or âŒ if not)
âœ… NPU model deployed (or â³ if not)
```

---

## ğŸ“ Expected File Structure

After everything:
```
HackNYU/
â”œâ”€â”€ models/
â”‚   â””â”€â”€ llama_npu/
â”‚       â””â”€â”€ model.onnx          â† Downloaded from AI Hub
â”‚
â”œâ”€â”€ deployed_models/
â”‚   â””â”€â”€ llama_npu/
â”‚       â””â”€â”€ compiled_model.onnx â† After deploy_fixed.py
â”‚
â””â”€â”€ talk_to_harry_npu.py        â† Your super-fast Harry!
```

---

## ğŸš€ Quick Reference

```powershell
# 1. Check status
python check_model_ready.py

# 2. Deploy (after download)
python deploy_fixed.py --model models\llama_npu\model.onnx

# 3. Chat with NPU Harry
python talk_to_harry_npu.py
```

---

## â“ Need Help?

**Model not downloading?**
- Try different search terms: "Llama quantized", "Llama 1B"
- Look in "Natural Language" category
- Make sure you select ONNX format!

**Deployment stuck?**
- This is normal! NPU compilation takes 10-30 minutes
- Don't cancel - let it run
- You can check status on: https://app.aihub.qualcomm.com/

**Deployment failed?**
- Check API key: `python check_device.py`
- Model might be too large - try smaller one
- Check error message for hints

---

## ğŸ‰ When Done

You'll have:
- âœ… Harry responding in ~500ms
- âœ… 12x faster than CPU
- âœ… Ready for full pipeline integration
- âœ… NPU-optimized for your device

**Next:** Integrate with wake word + STT + TTS for full <1s pipeline!

---

**Start now:** Open browser â†’ https://aihub.qualcomm.com/models â†’ Download Llama!

