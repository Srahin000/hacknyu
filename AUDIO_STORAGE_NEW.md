# Audio Storage System - UPDATED

## ğŸ“ New Organization (November 2025)

The voice assistant now uses a **clean separation** between TTS responses and user recordings:

```
audio/                                    â† TTS RESPONSES ONLY (Harry's voice)
â”œâ”€â”€ harry_tts_20251116_203045_conv0001.wav
â”œâ”€â”€ harry_tts_20251116_203045_conv0001.txt
â”œâ”€â”€ harry_tts_20251116_203152_conv0002.wav
â”œâ”€â”€ harry_tts_20251116_203152_conv0002.txt
â””â”€â”€ ...

conversations/                            â† FULL CONVERSATIONS (organized)
â”œâ”€â”€ 20251116/                             â† Date folder
â”‚   â”œâ”€â”€ conv_0001_203045/                 â† Conversation #1
â”‚   â”‚   â”œâ”€â”€ user_audio.wav                â† Your voice recording
â”‚   â”‚   â”œâ”€â”€ harry_audio.wav               â† Harry's TTS response
â”‚   â”‚   â”œâ”€â”€ transcript.txt                â† Full conversation + emotion
â”‚   â”‚   â”œâ”€â”€ harry_response.txt            â† Just Harry's text
â”‚   â”‚   â””â”€â”€ metadata.json                 â† Complete metadata + emotion data
â”‚   â”‚
â”‚   â”œâ”€â”€ conv_0002_203152/                 â† Conversation #2
â”‚   â”‚   â”œâ”€â”€ user_audio.wav
â”‚   â”‚   â”œâ”€â”€ harry_audio.wav
â”‚   â”‚   â”œâ”€â”€ transcript.txt
â”‚   â”‚   â”œâ”€â”€ harry_response.txt
â”‚   â”‚   â””â”€â”€ metadata.json
â”‚   â””â”€â”€ ...
â””â”€â”€ ...
```

## ğŸ¯ Key Changes

### Before (Confusing):
- âŒ Both user audio AND TTS audio in `audio/` folder
- âŒ Hard to tell which files are which
- âŒ No emotion detection

### After (Clean):
- âœ… `audio/` folder = **TTS responses ONLY** (Harry's voice)
- âœ… `conversations/` folder = **Complete conversations** (user + Harry + emotion)
- âœ… **Emotion detection** integrated (NPU-accelerated)

## ğŸ“ File Naming

### TTS Audio (in `audio/` folder):
```
harry_tts_YYYYMMDD_HHMMSS_conv####.wav
harry_tts_YYYYMMDD_HHMMSS_conv####.txt
```

Example:
- `harry_tts_20251116_203045_conv0001.wav` = Harry's TTS voice, Nov 16 2025, 8:30:45 PM
- `harry_tts_20251116_203045_conv0001.txt` = What Harry said (text)

### Conversation Files (in `conversations/YYYYMMDD/conv_####_HHMMSS/`):
```
user_audio.wav        â† Your voice recording
harry_audio.wav       â† Harry's TTS response (copy from audio/)
transcript.txt        â† Full conversation with emotion
harry_response.txt    â† Just Harry's response text
metadata.json         â† Complete conversation metadata
```

## ğŸ˜Š Emotion Detection

Every conversation now includes **emotion detection** on your voice!

### In `transcript.txt`:
```
Conversation #1
Timestamp: 2025-11-16 20:30:45
======================================================================

EMOTION: HAPPY (85% confidence)

USER:
Hello Harry, how are you?

HARRY:
Hello! I'm doing great, thanks for asking!
```

### In `metadata.json`:
```json
{
  "conversation_id": 1,
  "timestamp": "2025-11-16T20:30:45.123456",
  "user_query": "Hello Harry, how are you?",
  "harry_response": "Hello! I'm doing great, thanks for asking!",
  "emotion_type": "npu",
  "emotion": {
    "detected": "happy",
    "confidence": 0.85,
    "latency_ms": 42,
    "all_scores": {
      "happy": 0.85,
      "neutral": 0.08,
      "sad": 0.03,
      "angry": 0.02,
      "fear": 0.01,
      "disgust": 0.01,
      "surprise": 0.00
    }
  }
}
```

## ğŸ¤ Supported Emotions

The NPU emotion model detects 7 emotions:
1. **Happy** ğŸ˜Š
2. **Sad** ğŸ˜¢
3. **Angry** ğŸ˜ 
4. **Fear** ğŸ˜¨
5. **Disgust** ğŸ¤¢
6. **Surprise** ğŸ˜²
7. **Neutral** ğŸ˜

## ğŸ’¾ What Gets Saved

### For EVERY Conversation:

| Content | audio/ folder | conversations/ folder |
|---------|--------------|----------------------|
| **Your voice** | âŒ | âœ… `user_audio.wav` |
| **Harry's voice** | âœ… `harry_tts_*.wav` | âœ… `harry_audio.wav` (copy) |
| **Transcripts** | âœ… `harry_tts_*.txt` | âœ… `transcript.txt` + `harry_response.txt` |
| **Emotion data** | âŒ | âœ… In `transcript.txt` + `metadata.json` |
| **Metadata** | âŒ | âœ… `metadata.json` |

## ğŸš€ Usage Examples

### Get All TTS Audio Files
```powershell
# All Harry's voice responses
Get-ChildItem audio\harry_tts_*.wav

# Export for training
Copy-Item audio\harry_tts_*.wav -Destination D:\TTS_Training\
```

### Review a Conversation
```powershell
# View transcript with emotion
Get-Content conversations\20251116\conv_0001_203045\transcript.txt

# Play user audio
Start-Process conversations\20251116\conv_0001_203045\user_audio.wav

# Play Harry's response
Start-Process conversations\20251116\conv_0001_203045\harry_audio.wav

# Check metadata
Get-Content conversations\20251116\conv_0001_203045\metadata.json | ConvertFrom-Json
```

### Find Conversations by Emotion
```python
import json
from pathlib import Path

def find_by_emotion(target_emotion):
    """Find all conversations with a specific emotion"""
    results = []
    
    for metadata_file in Path("conversations").rglob("metadata.json"):
        with open(metadata_file) as f:
            data = json.load(f)
        
        if "emotion" in data and data["emotion"]["detected"] == target_emotion:
            results.append({
                "conversation": data["conversation_id"],
                "timestamp": data["timestamp"],
                "user_query": data["user_query"],
                "emotion_confidence": data["emotion"]["confidence"]
            })
    
    return results

# Find all happy conversations
happy_convos = find_by_emotion("happy")
for convo in happy_convos:
    print(f"Conv #{convo['conversation']}: {convo['user_query']}")
    print(f"  Confidence: {convo['emotion_confidence']*100:.0f}%")
```

## ğŸ“Š Storage Details

### Audio Quality:
- **User audio**: 16kHz mono WAV (Whisper input format)
- **Harry's audio**: 22kHz stereo WAV (XTTS v2 output)

### Disk Space (per conversation):
- User recording (8 sec): ~250 KB
- Harry's audio (varies): ~200-500 KB per response
- Metadata + text: ~5 KB

### Example Session (10 conversations):
- TTS audio in `audio/`: ~3-5 MB
- Full conversations: ~8-12 MB (includes user audio + TTS copies + metadata)

## âš™ï¸ Model Performance

### Emotion Detection:
- **NPU mode**: ~40-80ms latency per audio sample
- **Accuracy**: 97.46% on test dataset (Wav2Vec2)
- **Works offline**: No internet needed

### When Does Emotion Detection Run?
- After you finish speaking
- Before transcription starts
- Runs in parallel with Whisper STT (if on NPU)

## ğŸ”§ Troubleshooting

### Emotion detection not working?
```bash
# Test emotion model directly
python emotion_npu.py

# Check if model exists
ls models/emotion_wav2vec2/
```

### Clean up old audio files?
```powershell
# Remove old user audio files from audio/ folder (if you have any from before)
Remove-Item audio\user_*.wav
Remove-Item audio\user_*.txt
```

## ğŸ“š Related Files

- `harry_voice_assistant.py` - Main voice assistant with emotion integration
- `emotion_npu.py` - Emotion detection NPU wrapper
- `VOICE_ASSISTANT_GUIDE.md` - Complete voice assistant guide
- `README.md` - Project overview

